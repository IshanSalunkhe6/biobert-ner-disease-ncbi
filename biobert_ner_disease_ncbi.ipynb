{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x739K2B_pwh-"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers datasets seqeval evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Load NCBI Disease dataset\n",
        "dataset = load_dataset('ncbi_disease')\n",
        "\n",
        "split_dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    'train': split_dataset['train'],\n",
        "    'validation': split_dataset['test']\n",
        "})\n",
        "\n",
        "print(dataset)\n",
        "print(dataset['train'][0])\n",
        "print(dataset['validation'][0])\n"
      ],
      "metadata": {
        "id": "fUKH0oiqpxBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "# Load BioBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "\n",
        "# Load BioBERT model\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\", num_labels=3)\n"
      ],
      "metadata": {
        "id": "atbIXR4qpxEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize Dataset and Align Labels"
      ],
      "metadata": {
        "id": "WNcs6H6Xh5fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n"
      ],
      "metadata": {
        "id": "fZ6iQtVepxGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "2WvzhJXIpxJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# Load metric\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "# Labels list\n",
        "label_list = dataset['train'].features['ner_tags'].feature.names\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n"
      ],
      "metadata": {
        "id": "kyDy0P-XpxLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Training Arguments\n"
      ],
      "metadata": {
        "id": "o_rt9L0eh_8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./biobert_disease_ner\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "s-xmRDzLpxOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine-Tune BioBERT Model\n"
      ],
      "metadata": {
        "id": "w8lHyW1UiEj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Trainer\n",
        "from transformers import Trainer, DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "Uaag305xpxQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Evaluate final metrics\n",
        "metrics = trainer.evaluate()\n",
        "print(\"âœ… Final Evaluation Metrics:\")\n",
        "for key, value in metrics.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key}: {value:.4f}\")\n"
      ],
      "metadata": {
        "id": "omC0cMqvpxTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Training Loss Curve\n"
      ],
      "metadata": {
        "id": "Lmh2BVetiJjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract loss values\n",
        "training_loss = trainer.state.log_history\n",
        "\n",
        "# Get step vs loss\n",
        "steps = []\n",
        "losses = []\n",
        "\n",
        "for log in training_loss:\n",
        "    if \"loss\" in log:\n",
        "        steps.append(log[\"step\"])\n",
        "        losses.append(log[\"loss\"])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(steps, losses, marker='o')\n",
        "plt.title('Training Loss vs Steps')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5XegipINpxVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save and Upload the model to Hugging Face"
      ],
      "metadata": {
        "id": "R-BKmkFRhVSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlp = pipeline(\n",
        "    \"ner\",\n",
        "    model=\"Ishan0612/biobert-ner-disease-ncbi\",\n",
        "    tokenizer=\"Ishan0612/biobert-ner-disease-ncbi\",\n",
        "    aggregation_strategy=\"simple\"\n",
        ")\n",
        "\n",
        "text = \"The patient has signs of diabetes mellitus and chronic obstructive pulmonary disease.\"\n",
        "\n",
        "results = nlp(text)\n",
        "\n",
        "for entity in results:\n",
        "    print(f\"{entity['word']} ({entity['entity_group']}) - Confidence: {entity['score']:.2f}\")\n"
      ],
      "metadata": {
        "id": "fBVct27C_ea1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u12jJ737_enK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}